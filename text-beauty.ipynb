{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://www.github.com/keras-team/keras-contrib.git\r\n",
      "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-z505ccs5\r\n",
      "Requirement already satisfied: keras in /opt/conda/lib/python3.6/site-packages (from keras-contrib==2.0.8) (2.2.4)\r\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.6/site-packages (from keras->keras-contrib==2.0.8) (1.1.0)\r\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/conda/lib/python3.6/site-packages (from keras->keras-contrib==2.0.8) (1.0.7)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from keras->keras-contrib==2.0.8) (1.12.0)\r\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.6/site-packages (from keras->keras-contrib==2.0.8) (1.0.9)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from keras->keras-contrib==2.0.8) (2.9.0)\r\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.6/site-packages (from keras->keras-contrib==2.0.8) (1.16.2)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from keras->keras-contrib==2.0.8) (3.12)\r\n",
      "Building wheels for collected packages: keras-contrib\r\n",
      "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-nizmap_u/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\r\n",
      "Successfully built keras-contrib\r\n",
      "Installing collected packages: keras-contrib\r\n",
      "Successfully installed keras-contrib-2.0.8\r\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ndsc-beginner', 'test-fasttexttrain']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from gensim.models.fasttext import FastText\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import Sequence\n",
    "from keras.models import Model , load_model\n",
    "from keras import backend as K\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "import re\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from keras.layers import Bidirectional, CuDNNLSTM, Embedding , \\\n",
    "                            Input, GlobalMaxPooling1D, Conv1D, Lambda, \\\n",
    "                            Dense, Concatenate, Dropout, BatchNormalization,\\\n",
    "                            SpatialDropout1D, CuDNNGRU\n",
    "from keras.optimizers import Nadam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split , StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras_contrib.callbacks import CyclicLR\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "73d11d26967723541166895764efd44a576f00fe"
   },
   "outputs": [],
   "source": [
    "fasttextm = FastText.load('../input/test-fasttexttrain/textshopee.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "291f85955cf58a2391a0576400aef58244c99c07"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../input/ndsc-beginner/train.csv')\n",
    "test_data = pd.read_csv('../input/ndsc-beginner/test.csv')\n",
    "\n",
    "train_data = train_data[train_data['image_path'].str.contains('beauty')]\n",
    "test_data = test_data[test_data['image_path'].str.contains('beauty')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "f5fede011b61cf15a73ef2304bbb406056d09c88"
   },
   "outputs": [],
   "source": [
    "train_data['title'] = train_data['title'].apply(lambda s : re.sub(r'[^\\w\\s]','',s))\n",
    "train_data['title'] = train_data['title'].apply(lambda s : re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", s))\n",
    "test_data['title'] = test_data['title'].apply(lambda s : re.sub(r'[^\\w\\s]','',s))\n",
    "test_data['title'] = test_data['title'].apply(lambda s : re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "8dfd2d8f4fa4db566f773bbefe64500620203b84"
   },
   "outputs": [],
   "source": [
    "local_cat = {cat : i for i , cat in enumerate(train_data['Category'].unique())}\n",
    "inverse_local_cat = {cat : i for i , cat in enumerate(train_data['Category'].unique())}\n",
    "\n",
    "train_data['local_class'] = train_data['Category'].map(local_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "28d53fcf596b418f520576fc71ea26e4aab0c880"
   },
   "outputs": [],
   "source": [
    "train_text = train_data['title'].values\n",
    "train_label = train_data['local_class'].values\n",
    "\n",
    "test_text = test_data['title'].values\n",
    "\n",
    "n_classes = len(train_data['Category'].unique())\n",
    "#y_test = test_data['local_class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "92604dc4a86fd4ad8370c40d1a8e10910bff845a"
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 20000\n",
    "EMBED_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "58bc694dc97b3bbaa98cc233f53f72353b516942"
   },
   "outputs": [],
   "source": [
    "token = Tokenizer(MAX_NB_WORDS , char_level = False)\n",
    "token.fit_on_texts(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "74e69995ee20c8296c88abd817bf8f296604d54a"
   },
   "outputs": [],
   "source": [
    "word_index = token.word_index\n",
    "total_words = len(token.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "ef944b0ac2db12892e60b46ff011d1c7a1a29586"
   },
   "outputs": [],
   "source": [
    "sequences = token.texts_to_sequences(train_text)\n",
    "sequences = np.array(sequences)\n",
    "test_sequences = token.texts_to_sequences(test_text)\n",
    "test_sequences = np.array(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "f0723dd6887394711ece9936f47af19753eecfa3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad11df0a23744939fdc116ae078f351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=28598), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_WORDS = min(MAX_NB_WORDS, total_words)\n",
    "embedding_matrix = np.zeros((NUM_WORDS, EMBED_DIM))\n",
    "words_not_found = []\n",
    "\n",
    "for word, i in tqdm_notebook(word_index.items()):\n",
    "    if i >= NUM_WORDS:\n",
    "        break\n",
    "    try :\n",
    "        embedding_matrix[i] = fasttextm.wv[word]\n",
    "    except :\n",
    "        words_not_found.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "3c48383473991d4981b4ddda90915e0b41e57ebe"
   },
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence) :\n",
    "    def __init__(self , X_1 , labels = [] , shuffle = True, batch_size = 128) :\n",
    "        \n",
    "        ### Input X_1 list of tokenized titles\n",
    "        ### Input X_2 mentions_array\n",
    "        self.X_1 = np.array(X_1)\n",
    "        if len(labels) > 0 :\n",
    "            assert len(X_1) == len(labels)\n",
    "        self.labels = labels\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(X_1))\n",
    "        if self.shuffle :\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def on_epoch_end(self) :\n",
    "        if self.shuffle :\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __getitem__(self , idx) :\n",
    "        index = self.indexes[idx*self.batch_size :min(len(self.X_1) , (idx+1)*self.batch_size)]\n",
    "        curr_batch_X_1 = pad_sequences(self.X_1[index])\n",
    "        #curr_batch_X_2 = self.X_2[index]\n",
    "        if len(self.labels) :\n",
    "            curr_batch_labels = self.labels[index]\n",
    "            return curr_batch_X_1 , curr_batch_labels\n",
    "        else :\n",
    "            return curr_batch_X_1\n",
    "            \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X_1) / self.batch_size))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "beb60b32d1f9e61ce50bd3245468f59bc4e26e4d"
   },
   "outputs": [],
   "source": [
    "def gen_model(n_classes = 2) :\n",
    "    inp = Input(shape=(None,))\n",
    "    x = Embedding(NUM_WORDS, EMBED_DIM, weights=[embedding_matrix], trainable=False, name='EMBEDDING')(inp)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    x1 = Bidirectional(CuDNNLSTM(384, return_sequences=True))(x)\n",
    "    x2 = Bidirectional(CuDNNGRU(256, return_sequences=True))(x1)\n",
    "    x1 = Bidirectional(CuDNNLSTM(256, return_sequences=True))(x1)\n",
    "    max_pool1 = GlobalMaxPooling1D()(x1)\n",
    "    max_pool2 = GlobalMaxPooling1D()(x2)\n",
    "    conc = Concatenate()([max_pool1, max_pool2])\n",
    "    conc = Dropout(0.3)(conc)\n",
    "    \n",
    "    ###### Trial\n",
    "    x_1 = Dense(n_classes , activation = 'softmax')(conc)\n",
    "    m = Model(inp , x_1)\n",
    "\n",
    "    m.compile(loss = 'sparse_categorical_crossentropy' , optimizer = 'Nadam' , metrics = ['accuracy'])\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "f79fecf07c969a1317953b5984b75df38b633416"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "n_split = 8\n",
    "kf = StratifiedKFold(n_split,  random_state = 100)\n",
    "models = [gen_model(n_classes) for i in range(n_split)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "6a36875917b48ae7c1f3811afebb99f17c3285e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/30\n",
      "980/980 [==============================] - 47s 48ms/step - loss: 0.8999 - acc: 0.7278 - val_loss: 0.7875 - val_acc: 0.7598\n",
      "Epoch 2/30\n",
      "980/980 [==============================] - 43s 44ms/step - loss: 0.7931 - acc: 0.7543 - val_loss: 0.7468 - val_acc: 0.7684\n",
      "Epoch 3/30\n",
      "980/980 [==============================] - 43s 43ms/step - loss: 0.7520 - acc: 0.7638 - val_loss: 0.7268 - val_acc: 0.7721\n",
      "Epoch 4/30\n",
      "980/980 [==============================] - 43s 43ms/step - loss: 0.7273 - acc: 0.7702 - val_loss: 0.7107 - val_acc: 0.7776\n",
      "Epoch 5/30\n",
      "980/980 [==============================] - 42s 43ms/step - loss: 0.7097 - acc: 0.7756 - val_loss: 0.7041 - val_acc: 0.7813\n",
      "Epoch 6/30\n",
      "980/980 [==============================] - 42s 43ms/step - loss: 0.6991 - acc: 0.7772 - val_loss: 0.7051 - val_acc: 0.7803\n",
      "Epoch 7/30\n",
      "980/980 [==============================] - 43s 43ms/step - loss: 0.6950 - acc: 0.7783 - val_loss: 0.6957 - val_acc: 0.7840\n",
      "Epoch 8/30\n",
      "980/980 [==============================] - 42s 43ms/step - loss: 0.6913 - acc: 0.7793 - val_loss: 0.7022 - val_acc: 0.7835\n",
      "Epoch 9/30\n",
      "980/980 [==============================] - 42s 43ms/step - loss: 0.6930 - acc: 0.7795 - val_loss: 0.6946 - val_acc: 0.7836\n",
      "Epoch 10/30\n",
      "980/980 [==============================] - 42s 43ms/step - loss: 0.6954 - acc: 0.7780 - val_loss: 0.6977 - val_acc: 0.7842\n",
      "Epoch 11/30\n",
      "980/980 [==============================] - 42s 43ms/step - loss: 0.7008 - acc: 0.7777 - val_loss: 0.7008 - val_acc: 0.7813\n",
      "Epoch 12/30\n",
      "980/980 [==============================] - 43s 44ms/step - loss: 0.7059 - acc: 0.7761 - val_loss: 0.7104 - val_acc: 0.7810\n",
      "Epoch 13/30\n",
      "980/980 [==============================] - 43s 44ms/step - loss: 0.7134 - acc: 0.7742 - val_loss: 0.7049 - val_acc: 0.7804\n",
      "Epoch 14/30\n",
      " 53/980 [>.............................] - ETA: 37s - loss: 0.6781 - acc: 0.7827"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "for i , (train_idx , test_idx) in enumerate(kf.split(sequences, train_label)) :\n",
    "    train_generator = DataGenerator(sequences[train_idx] , train_label[train_idx], shuffle = True, batch_size = 256)\n",
    "    val_generator = DataGenerator(sequences[test_idx] , train_label[test_idx] , shuffle = False)\n",
    "    cb = [\n",
    "        #ModelCheckpoint('model_{}.h5'.format(i) , save_best_only =True , monitor = 'val_loss', verbose = True),\n",
    "        CyclicLR(5e-4 , 0.004 , int(epochs*len(train_generator)/2) )\n",
    "    ]\n",
    "    hist = models[i].fit_generator(train_generator, \n",
    "                                epochs = epochs, \n",
    "                                validation_data = val_generator,\n",
    "                                verbose = 1,\n",
    "                                callbacks = cb).history\n",
    "    \n",
    "    models[i].layers[1].trainable = True\n",
    "    models[i].compile(loss = 'sparse_categorical_crossentropy' , optimizer = Nadam(5e-4) , metrics = ['accuracy'])\n",
    "    models[i].fit_generator(train_generator, \n",
    "                                epochs = 5, \n",
    "                                validation_data = val_generator,\n",
    "                                verbose = 1,\n",
    "                                callbacks = cb).history\n",
    "    \n",
    "    print('Epochs', i , 'val acc :' , hist['val_acc'][-1] , '     val loss :', hist['val_loss'][-1])\n",
    "    print('         train acc :', hist['acc'][-1]  , '   train loss :' , hist['loss'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "0a1652a300f864924e7564505f5d3f5ea5c33d1b"
   },
   "outputs": [],
   "source": [
    "test_generator = DataGenerator(test_sequences , shuffle = False)\n",
    "train_pred_generator = DataGenerator(sequences, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "1fe396aa333e1f13becc9a1e2c12aaa6d8bb52a0"
   },
   "outputs": [],
   "source": [
    "pred_train = np.zeros([len(train_data) , n_classes])\n",
    "pred_test = np.zeros([len(test_data) , n_classes])\n",
    "for i in range(n_split) :\n",
    "    pred_train += models[i].predict_generator(train_pred_generator)\n",
    "    pred_test += models[i].predict_generator(test_generator)\n",
    "    \n",
    "pred_train /= n_split\n",
    "pred_test /= n_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "154a8810f17e3a901a88fbb480cae29d2990d141"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "sub_pred_df = test_data[['itemid']]\n",
    "for i in range(n_classes) :\n",
    "    sub_pred_df['{}'.format(i)] = pred_test[:,i]\n",
    "sub_train_df = train_data[['itemid']]\n",
    "for i in range(n_classes) :\n",
    "    sub_train_df['{}'.format(i)] = pred_train[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "79524ccc2106db760595b62f2fe35674f0b10bea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training Accuracy : 0.8314066082077444\n"
     ]
    }
   ],
   "source": [
    "print('Model Training Accuracy :' , accuracy_score(train_label , pred_train.argmax(axis = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "796626fb76ceb868a1ddd55eab3333cdf5f321c3"
   },
   "outputs": [],
   "source": [
    "sub_pred_df.to_csv('sub_beauty.csv' , index = False)\n",
    "sub_train_df.to_csv('sub_train_beauty.csv' , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "a1604468551549f049bf16eda48eca37aed5f81b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
